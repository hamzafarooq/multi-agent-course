{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_3/Semantic_Cache/Semantic_cache_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you use our code, please cite:**\n",
        "\n",
        "@misc{2024<br>\n",
        "  title = {Semantic Cache from Scratch},<br>\n",
        "  author = {Hamza Farooq, Darshil Modi, Kanwal Mehreen, Nazila Shafiei},<br>\n",
        "  keywords = {Semantic Cache},<br>\n",
        "  year = {2024},<br>\n",
        "  copyright = {APACHE 2.0 license}<br>\n",
        "}"
      ],
      "metadata": {
        "id": "p2KzXaJYks8S"
      },
      "id": "p2KzXaJYks8S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Cache\n",
        "\n",
        "Semantic caching accelerates retrieval-augmented workflows by storing and reusing previous embedding-based lookups instead of issuing fresh queries every time. In this notebook, we’ll build a lightweight semantic cache from scratch using:\n",
        "\n",
        "- **Nomic text embeddings** (`nomic-ai/nomic-embed-text-v1.5`) to convert documents and queries into dense vectors  \n",
        "- **FAISS** (Facebook AI Similarity Search) to index and quickly search those vectors  \n",
        "- A **ground-truth evaluation** dataset to measure cache hit/miss accuracy  \n",
        "- **Traversaal Ares API** to fetch live data when cache misses require real-time information  \n",
        "\n",
        "Rather than re-computing embeddings and retrieval for every query, our cache lets us:\n",
        "\n",
        "1. **Embed** a corpus once and index it for fast L2 nearest-neighbor lookup  \n",
        "2. **Embed** each new query and check if it’s already “covered” by a cached result  \n",
        "3. **Fall back** to a full retrieval (and store the new result) only when necessary  \n",
        "4. **Invoke** the Traversaal Ares API for live internet search when needed  \n",
        "\n",
        "This approach reduces redundant compute, lowers end-to-end latency, and makes RAG pipelines more efficient—especially when query patterns exhibit repetition, temporal locality, or high similarity. We’ll walk through:\n",
        "\n",
        "1. Loading the Nomic embed model with `trust_remote_code=True`  \n",
        "2. Encoding a document set and building a FAISS index  \n",
        "3. Loading a real-world ground-truth CSV for evaluation  \n",
        "4. Implementing the core cache hit/miss logic  \n",
        "5. Falling back to Traversaal Ares API for live data on cache misses  \n",
        "6. Measuring performance gains against a “no-cache” baseline  \n",
        "\n",
        "By the end, you’ll have a reusable semantic cache scaffold that you can plug into any RAG or search-over-embeddings pipeline. Let’s get started!  \n"
      ],
      "metadata": {
        "id": "gxQxLQJymkCl"
      },
      "id": "gxQxLQJymkCl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies"
      ],
      "metadata": {
        "id": "Hm2NYbYQmykl"
      },
      "id": "Hm2NYbYQmykl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install -U faiss-cpu sentence_transformers transformers"
      ],
      "metadata": {
        "id": "025_hZMnZUIE"
      },
      "id": "025_hZMnZUIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52273bc0-575b-4007-b63d-bfe53d4abde6",
      "metadata": {
        "id": "52273bc0-575b-4007-b63d-bfe53d4abde6"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "\n",
        "# FAISS for efficient similarity search over vector embeddings\n",
        "import faiss  # Builds and queries approximate nearest neighbor indices\n",
        "\n",
        "# Lightweight SQL database for caching metadata, query logs, or evaluation results\n",
        "import sqlite3  # Persistence layer for storing cache entries or metrics\n",
        "\n",
        "# SentenceTransformers wrapper around transformer models for text embeddings\n",
        "from sentence_transformers import SentenceTransformer  # Loads Nomic/embed or other SBERT-style models\n",
        "\n",
        "# PyTorch backend required by SentenceTransformer and optional model fine-tuning\n",
        "import torch  # Tensor operations, GPU acceleration, and model inference support\n",
        "\n",
        "# Transformers library components for causal LLM-based answer generation\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#   - AutoModelForCausalLM: Load pretrained language models (e.g., GPT variants)\n",
        "#   - AutoTokenizer: Tokenize text input/output for the LLM\n",
        "\n",
        "# Core numerical library for array and matrix operations on embeddings\n",
        "import numpy as np  # Handles vector math, concatenation, and statistical computations\n",
        "\n",
        "# Pretty-printing complex Python objects during development/debugging\n",
        "from pprint import pprint  # Nicely formats nested dicts or lists when exploring outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Define prediction function (Using Traversaal Ares API)\n",
        "\n",
        "Instead of using an LLM endpoint, we will be using Ares API for retrieval and generation, however you can replace is with your own rag function in 'generate answer' function\n",
        "\n",
        "Traversaal Ares API is a cutting-edge solution designed to provide real-time search results generated from user queries. Leveraging advanced Large Language Models (LLMs), Ares connects to the internet to deliver accurate and factual information, including relevant URLs for reference. This API is tailored for speed and efficiency, providing lightning-fast search results within 3-4 seconds. Currently available for free during the beta phase, with priced solutions coming soon.\n",
        "\n",
        "## Key Features:\n",
        "- **Real-time Search Results:** Ares API offers unparalleled speed in generating search results.\n",
        "- **Internet Connectivity:** Connects to the internet to fetch the latest and most accurate information.\n",
        "- **Lightning-Fast Response:** Delivers search results with URLs in 3-4 seconds.\n",
        "- **Free Beta Access:** Available for free during for the first 100 calls\n",
        "- **Factual and Accurate:** Ensures the information provided is accurate and supported by relevant references. [Can make mistakes though]\n",
        "\n",
        "## Getting Started:\n",
        "To access the Ares API, sign up at [api.traversaal.ai](https://api.traversaal.ai) and refer to the usage documentation at [docs.traversaal.ai](https://docs.traversaal.ai/docs/intro).\n",
        "\n",
        "Experience the future of AI-driven search with Traversaal Ares API!\n"
      ],
      "metadata": {
        "id": "4JHRRiuKlM3t"
      },
      "id": "4JHRRiuKlM3t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38331891-adb4-4d16-b26f-d74d7c9ce728",
      "metadata": {
        "id": "38331891-adb4-4d16-b26f-d74d7c9ce728"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata # Colab utility for securely storing/retrieving credentials\n",
        "import requests # HTTP client for REST API calls\n",
        "\n",
        "def make_prediction(data):\n",
        "    \"\"\"\n",
        "    Send a text query to the Traversaal Ares live-predict endpoint and return parsed JSON.\n",
        "\n",
        "    Args:\n",
        "        data (str): The user query string to send for prediction.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Parsed JSON response from Ares API if successful, else None.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If ARES_API_KEY is missing.\n",
        "    \"\"\"\n",
        "    url = \"https://api-ares.traversaal.ai/live/predict\"\n",
        "    # Retrieve API key from Colab secure storage; fail fast if missing\n",
        "    api_key = userdata.get(\"ARES_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Missing ARES_API_KEY in Colab userdata\")\n",
        "    headers = {\n",
        "        \"x-api-key\": api_key,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # The ARES API expects a JSON body of the form {\"query\": <text>}\n",
        "    payload = {\"query\": data}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        # Check for HTTP 200 OK\n",
        "        if response.status_code == 200:\n",
        "            print(\"Request was successful.\")\n",
        "            try:\n",
        "                 # If the response contains JSON data, you can parse it using response.json()\n",
        "                return response.json()\n",
        "            except ValueError:\n",
        "                # Response text wasn’t valid JSON\n",
        "                print(\"No JSON data in the response.\")\n",
        "                return None\n",
        "        else:\n",
        "            # Non-200 status; surface the code for debugging\n",
        "            print(f\"Request failed with status code {response.status_code}.\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the make_prediction function\n",
        "response=make_prediction(['Events happening in London this week. '])\n",
        "response"
      ],
      "metadata": {
        "id": "QC027Sholey1"
      },
      "id": "QC027Sholey1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define SemanticCaching Class\n",
        "\n",
        "In this cell we define `SemanticCaching`—a lightweight cache that:\n",
        "\n",
        "1. Uses **FAISS** to index and lookup question embeddings (Euclidean distance).  \n",
        "2. Leverages **Nomic Embed** (`nomic-ai/nomic-embed-text-v1.5`) to encode questions into vectors.  \n",
        "3. Persists cache entries (questions, embeddings, answers) in a JSON file.  \n",
        "4. Falls back to `make_prediction()` (via Traversaal Ares API) when no suitable cache hit is found.  \n",
        "5. Measures and logs query latency for both hits and misses.  \n"
      ],
      "metadata": {
        "id": "db-RrmtuqFhC"
      },
      "id": "db-RrmtuqFhC"
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss            # Efficient similarity search over vector embeddings\n",
        "import json             # Read/write cache from a JSON file\n",
        "import numpy as np      # Numerical operations on embeddings\n",
        "from sentence_transformers import SentenceTransformer  # Load Nomic embed model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM  # (Optional) LLM for answer gen\n",
        "import time             # Measure latency\n",
        "\n",
        "class SemanticCaching:\n",
        "    def __init__(self, json_file='cache.json'):\n",
        "        # Initialize Faiss index with Euclidean distance\n",
        "        self.index = faiss.IndexFlatL2(768)  # Use IndexFlatL2 with Euclidean distance\n",
        "        if self.index.is_trained:\n",
        "            print('Index trained')\n",
        "\n",
        "        # Initialize Sentence Transformer model\n",
        "        self.encoder = SentenceTransformer('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True) #to be replaced with nomic\n",
        "\n",
        "\n",
        "        # Uncomment the following lines to use DialoGPT for question generation\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "        # self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "        # Euclidean distance threshold for cache hits (lower = more similar)\n",
        "        self.euclidean_threshold = 0.3\n",
        "\n",
        "        # JSON file to persist cache entries\n",
        "        self.json_file = json_file\n",
        "        self.load_cache()\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Load existing cache or initialize empty structure.\"\"\"\n",
        "        try:\n",
        "            with open(self.json_file, 'r') as file:\n",
        "                self.cache = json.load(file)\n",
        "        except FileNotFoundError:\n",
        "          # Structure: lists of questions, embeddings, answers, and full response text\n",
        "            self.cache = {'questions': [], 'embeddings': [], 'answers': [], 'response_text': []}\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Persist cache back to disk.\"\"\"\n",
        "        with open(self.json_file, 'w') as file:\n",
        "            json.dump(self.cache, file)\n",
        "\n",
        "    def ask(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Returns a cached answer if within threshold, otherwise generates,\n",
        "        caches, and returns a new answer.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            # Encode the incoming question\n",
        "            l = [question]\n",
        "            embedding = self.encoder.encode(l)\n",
        "\n",
        "            # Search for the nearest neighbor in the index\n",
        "            D, I = self.index.search(embedding, 1)\n",
        "\n",
        "            # 3) If a neighbor exists and is within threshold → cache hit\n",
        "            if D[0] >= 0:\n",
        "                if I[0][0] != -1 and D[0][0] <= self.euclidean_threshold:\n",
        "                    row_id = int(I[0][0])\n",
        "                    print(f'Cache hit at row: {row_id} with score {1 - D[0][0]}') #score inversed to show similarity\n",
        "                    print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
        "                    return self.cache['response_text'][row_id]\n",
        "\n",
        "            # Handle the case when there are not enough results or Euclidean distance is not met\n",
        "            answer, response_text = self.generate_answer(question)\n",
        "\n",
        "            # Append new entry to cache\n",
        "            self.cache['questions'].append(question)\n",
        "            self.cache['embeddings'].append(embedding[0].tolist())\n",
        "            self.cache['answers'].append(answer)\n",
        "            self.cache['response_text'].append(response_text)\n",
        "            self.index.add(embedding)\n",
        "            self.save_cache()\n",
        "            print(f\"Time taken: {time.time() - start_time:.3f}s\")\n",
        "\n",
        "            return response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'ask' method: {e}\")\n",
        "\n",
        "    def generate_answer(self, question: str) -> str:\n",
        "        \"\"\"\n",
        "        Always use the Traversaal Ares API for new answers.\n",
        "        Returns (full API result dict, extracted response_text).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = make_prediction(question)\n",
        "\n",
        "            if not result or 'data' not in result or 'response_text' not in result['data']:\n",
        "                print(f\"[Warning] Incomplete or missing response for: {question}\")\n",
        "                fallback = \"No answer available due to API failure.\"\n",
        "                return {\"error\": True}, fallback\n",
        "\n",
        "            return result, result['data']['response_text']\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during 'generate_answer' method: {e}\")"
      ],
      "metadata": {
        "id": "yDHhY-OBSEIw"
      },
      "id": "yDHhY-OBSEIw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0",
      "metadata": {
        "id": "dc661dab-f7cc-4d74-9575-1c756b4cdef0"
      },
      "outputs": [],
      "source": [
        "# Instantiate the semantic cache: builds/loads FAISS index, encoder, and JSON cache\n",
        "cache = SemanticCaching()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Semantic Cache\n",
        "\n",
        "In this section, we validate the behavior of our `SemanticCaching` class using a small set of example questions. We’ll loop through three queries:\n"
      ],
      "metadata": {
        "id": "noc3G8Abxy2t"
      },
      "id": "noc3G8Abxy2t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e",
      "metadata": {
        "id": "2f64fe4d-fe89-44a7-bf3f-3ad721985f3e"
      },
      "outputs": [],
      "source": [
        "# First test question\n",
        "question1 = \"What is the capital of France?\"\n",
        "answer1 = cache.ask(question1)  # Cache miss: generates new answer via API and stores it\n",
        "print(answer1)\n",
        "\n",
        "# Second test question\n",
        "question2 = \"Who is the CEO of Apple?\"\n",
        "answer2 = cache.ask(question2)  # Cache miss: generates answer, stores embedding + response\n",
        "print(answer2)\n",
        "\n",
        "# Third test question\n",
        "question3 = \"Who is the CEO of Facebook?\"\n",
        "answer3 = cache.ask(question3)  # Cache hit or miss depends on similarity threshold\n",
        "print(answer3)\n",
        "\n",
        "# Fourth test question\n",
        "question4 = \"What is the capital of India?\"\n",
        "answer4 = cache.ask(question4)\n",
        "print(answer4)\n",
        "\n",
        "# Note:\n",
        "# If question3 is found similar enough (within threshold) to question2, it returns cached answer2.\n",
        "# Otherwise, it generates a fresh answer and adds it to the cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00",
      "metadata": {
        "id": "5eade92a-a4f7-406f-85d3-ae24146d9c00"
      },
      "outputs": [],
      "source": [
        "print(cache.ask(\"Can you tell me what is the Capital of India\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067af075-1df3-4fa7-90bf-52b14d819406",
      "metadata": {
        "id": "067af075-1df3-4fa7-90bf-52b14d819406"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Who is the CEO of Facebook?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9a6a23-83d7-4688-b037-fc015f295e83",
      "metadata": {
        "id": "7b9a6a23-83d7-4688-b037-fc015f295e83"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Who is the current CEO of Google?'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cache.ask('Is Sundar Pichai the CEO of Google?'))"
      ],
      "metadata": {
        "id": "2P3Tso8TTElH"
      },
      "id": "2P3Tso8TTElH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015dd13f-9de9-409b-9273-6730fe173585",
      "metadata": {
        "id": "015dd13f-9de9-409b-9273-6730fe173585"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in Edinburgh for a couple?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1",
      "metadata": {
        "id": "2cf696d0-2660-4cae-99b1-583807e7e5f1"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in Edinburgh?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a",
      "metadata": {
        "id": "b4e94626-1fd1-4493-8b8f-9550a1460e7a"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in London?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8dd316-d0d4-490f-9a9b-21f29a29c6ba",
      "metadata": {
        "id": "3f8dd316-d0d4-490f-9a9b-21f29a29c6ba"
      },
      "outputs": [],
      "source": [
        "print(cache.ask('Best local food spots in London?'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}